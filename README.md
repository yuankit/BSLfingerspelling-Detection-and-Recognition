# British Sign Language (BSL) Fingerspelling Detection and Recognition
The Tensorflow Keras model is trained with synthetic images generated by combinations of hand sign images and background images. Custom loss function (Distance IoU loss) and metrics (detection (of presence) accuracy, classification accuracy and IoU) are incorporated to train and evaluate the model.

## Preprocessing of images ##
### Hand sign images ###
1) At least 25 images of an alphabet fingerspelling sign are taken with phone (in high resolution, at least 1000 x 1000). Images are taken with the following consideration:
    a. Orientation
    b. brightness
    c. Background
2) The images are segmented with GrabCut Algorithm

### Background images ###
1) The background images dataset is obtained by a combination of (i) web scrapping from Unsplash, (ii) Manually selecting from Unsplash and (iii) Taking the image personally with webcam  
    a. High resolution images scrapped from Unplash (with the keyword and num. of images below)  
      i. Background office - 200  
      ii. Colour - 200  
      iii. Fashion - 1000  
      iv. Female - 200  
      v. Indoor - 200  
      vi. Kids - 200  
      vii. Male - 200  
      viii. People - 1000  
      
    b. High resolution images manually selected from Unsplaash (with the keyword and num. of images below)
      i. Cluttered - 30
      ii. Patterned - 30
      iii. Pure Colour - 40
      iv. Random places - 40
    c. Personally taking them with webcam
      i. 82 images
2) A total of 3422 background images, 3250 (95%) for training set, 172 (5%) for validation set
3) Background images are median blurred and resized to 350 x 350 (Larger than the model input and output image size of 300 x 300, to allow greater variation of background images when cropped later)

### Synthetic image generator ###
Works by first defining a black image that will be replaced by a random fingerspelling sign image pasting on a random background (details found at *"image_generator"* function of *"main_full_extend.ipynb"*):
1) Picks a random backgorund image and randomly crops a 300 x 300 image from it
    a. 0.5 of batch size is flipped horizontally to allow greater variations
2) Hand image is set to appear for 0.85 of the batch size
    a. The hand images of the entire batch are resized with resize factor between 0.45 and 0.65
3) A mask of the hand image is created and pasted into the background:
    a. Crop a mask from the background image exactly at the hand location
    b. Produce a hand mask (binary image) of just "True" and "False" (Hand pixels are now represented with value "False" whereas the black background is represented with "True") --> similar to a silhouette
    c. Create a new mask by multiplication of the the hand mask with background mask (the outline of the hand is now segmented, the pixels correspond to the hand is now black/has 0 pixel value), followed by sum of the hand image and the new mask (Recall taht the hand image has black/0 pixel value background)
    d. Paste the mask back into the background image
4) For a specific fraction of the synthetically generated images, all of below/some transformaitons are carried out depending on a predefined probability:
   a. Change of brightness (0.8 of batch size)
   b. A transformation that converts the image RGB space into RGB-equivalent of HSV space where the image becomes independent of the brightness ("V" of "HSV")
   
Example:
![image](https://user-images.githubusercontent.com/81301185/161438873-65420cd8-5459-4bc7-bcb5-f0fc047e9128.png)


## Results ##
![image](https://user-images.githubusercontent.com/81301185/161439361-ffad6253-2711-4cce-b9eb-44db056afb0b.png)
NOTE: Images with no handsign present do not affect classification accuracy and IoU metrics during the training and validating process. Only the detection/presence accuracy is affected.
