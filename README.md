# British Sign Language (BSL) Fingerspelling Detection and Recognition
## Project Summary ##
* Built a BSL fingerspelling detection, recognition and localisation system, mainly using Tensorflow 2 and OpenCV.
* Constructed an image generator capable of producing virtually unlimited synthetic images, from combinations of ~650 masks of personally taken fingerspelling images and ~3000 web-scrapped (Selenium, Requests, RegEx) background images, followed by image augmentation via image processing techniques (NumPy).
* Implemented GrabCut algorithm to create mask of hand sign images, as input to the image generator.
* Incorporated transfer learning to train neural network model (VGG16/ResNet50) with custom made loss functions and metrics.


The model (built on top of vgg16 model using Tensorflow Keras module) is trained with synthetic images generated by combinations of hand sign images and background images. Custom loss function (Distance IoU loss) and metrics (detection/presence accuracy, classification accuracy and IoU) are implemented to train and evaluate the model.

## Preprocessing of images ##
### Hand sign images ###
1) At least 25 images of an alphabet fingerspelling sign are taken with phone (in high resolution, at least 1000 x 1000). Images are taken with the following consideration:  
    a. Orientation  
    b. Brightness  
    c. Background  
2) A total of 720 fingerspelling images are taken, 684 (95%) for training set, 36 (5%) for testing set.  
2) The images are segmented with [GrabCut Algorithm](https://github.com/imseeom/Hand-Segmentation-with-GrabCut-Algo). 

### Background images ###
1) The background images dataset is obtained by a combination of (i) web scrapping from Unsplash, (ii) Manually selecting from Unsplash and (iii) Taking the image personally with webcam  
    a. High resolution images scrapped from Unplash (with the keyword and num. of images below). Details can be found in [this repository](https://github.com/imseeom/Image-Scrapping-Unsplash)  
        i. Background office - 200  
        ii. Colour - 200  
        iii. Fashion - 1000  
        iv. Female - 200  
        v. Indoor - 200  
        vi. Kids - 200  
        vii. Male - 200  
        viii. People - 1000  
      
    b. High resolution images manually selected from Unsplaash (with the keyword and num. of images below)  
      i. Cluttered - 30  
      ii. Patterned - 30  
      iii. Pure Colour - 40  
      iv. Random places - 40  
    c. Personally taking them with webcam  
      i. 82 images  
2) A total of 3422 background images, 3250 (95%) for training set, 172 (5%) for testing set
3) Background images are median blurred and resized to 350 x 350 (Larger than the model input and output image size of 300 x 300, to allow greater variation of background images when cropped later)

### Synthetic image generator ###
Works by first defining a black image that will be replaced by a random fingerspelling sign image pasting on a random background (details found at *"image_generator"* function of *"main_full_extend.ipynb"*):
1) Picks a random backgorund image and randomly crops a 300 x 300 image from it
    a. 0.5 of batch size is flipped horizontally to allow greater variations
2) Hand image is set to appear for 0.85 of the batch size
    a. The hand images of the entire batch are resized with resize factor between 0.45 and 0.65
3) A mask of the hand image is created and pasted into the background:
    a. Crop a mask from the background image exactly at the hand location
    b. Produce a hand mask (binary image) of just "True" and "False" (Hand pixels are now represented with value "False" whereas the black background is represented with "True") --> similar to a silhouette
    c. Create a new mask by multiplication of the the hand mask with background mask (the outline of the hand is now segmented, the pixels correspond to the hand is now black/has 0 pixel value), followed by sum of the hand image and the new mask (Recall taht the hand image has black/0 pixel value background)
    d. Paste the mask back into the background image
4) For a specific fraction of the synthetically generated images, all of below/some transformaitons are carried out depending on a predefined probability:
   a. Change of brightness (0.8 of batch size)
   b. A transformation that converts the image RGB space into RGB-equivalent of HSV space where the image becomes independent of the brightness ("V" of "HSV")
   
Example:
![image](https://user-images.githubusercontent.com/81301185/161438873-65420cd8-5459-4bc7-bcb5-f0fc047e9128.png)


## Results ##
![image](https://user-images.githubusercontent.com/81301185/161439361-ffad6253-2711-4cce-b9eb-44db056afb0b.png)
NOTE: Images with no handsign present do not affect classification accuracy and IoU metrics during the training and validating process. Only the detection/presence accuracy is affected.
![image](https://user-images.githubusercontent.com/81301185/161462007-65145180-8fc6-42ad-a91e-5b6cad4453d3.png)
![image](https://user-images.githubusercontent.com/81301185/161462105-9d070c99-9ff6-4c6e-98b2-7154847f6a5f.png)
- As expected, the model performs worse when tested with real images as compared to the synthetic images, especially in terms of its classification accuracy. IoU is still pretty decent even when tested with the real images.
- As compared to similar model but trained on just the real images, the model trained on synthetic data performs much better, especially in real-time prediction, possibly due to the larger dataset of greater variation of images as well as minimial overfitting.
- The project is carried out with the objectives of exploring and learning image processing, computer vision techniques as well as systems involving neural networks. The code is more of a collections of notes and thoughts instead of being presentable. Although efforts have been made to fine-tune the model and to introduce variety in the image dataset such that they resemble actual real-life images more closely, the image dataset may not be the best representation of actual images of hand sign.
